{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Seq2Seq_with_LSTM.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyMA+KJKlZOs/MiLCRkOv/qI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Seq2Seq Learning with Neural Networks (NIPS 2014) 실습"],"metadata":{"id":"-SI1XOuxeq7R"}},{"cell_type":"markdown","source":["### 1. 데이터 전처리\n","  - spaCy library: 문장의 토큰화, 태깅 등의 전처리 기능을 위한 라이브러리"],"metadata":{"id":"PuM5dg3mhBjL"}},{"cell_type":"code","source":["%%capture\n","!python -m spacy download en\n","!python -m spacy download de"],"metadata":{"id":"gZ-5RVGhe1H_","executionInfo":{"status":"ok","timestamp":1648193218094,"user_tz":-540,"elapsed":9829,"user":{"displayName":"윤세영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08877759417780914923"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["import spacy\n","\n","# 영어 및 독일어 토큰화\n","spacy_en = spacy.load('en')\n","spacy_de = spacy.load('de')"],"metadata":{"id":"9P1EXwFifFIY","executionInfo":{"status":"ok","timestamp":1648193224430,"user_tz":-540,"elapsed":2273,"user":{"displayName":"윤세영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08877759417780914923"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["# 간단한 토큰화 수행\n","tokenized = spacy_en.tokenizer('I am a graduate student.')\n","\n","for idx, token in enumerate(tokenized):\n","  print(f'index {idx}: {token.text}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KZqEa00MfO5C","executionInfo":{"status":"ok","timestamp":1648193227487,"user_tz":-540,"elapsed":311,"user":{"displayName":"윤세영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08877759417780914923"}},"outputId":"df418ab7-2089-482b-e73b-e0ea092f363d"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["index 0: I\n","index 1: am\n","index 2: a\n","index 3: graduate\n","index 4: student\n","index 5: .\n"]}]},{"cell_type":"code","source":["# 영어 및 독일어 토큰화 함수 정의\n","\n","# 독일어 문장 (입력 시퀀스) 토큰화 한 뒤, 순서 뒤집기\n","# 논문에서 인코더를 거꾸로 뒤집을 때 성능이 좋게 나왔다고 함\n","def tokenize_de(text):\n","  return [token.text for token in spacy_de.tokenizer(text)][::-1]\n","\n","# 영어 문장 (출력 시퀀스) 토큰화 함수\n","def tokenize_en(text):\n","  return [token.text for token in spacy_en.tokenizer(text)]"],"metadata":{"id":"YEZxrLZPflyj","executionInfo":{"status":"ok","timestamp":1648193231531,"user_tz":-540,"elapsed":6,"user":{"displayName":"윤세영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08877759417780914923"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["!pip install -U torchtext==0.8.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6IaheMUfpGfg","executionInfo":{"status":"ok","timestamp":1648193236585,"user_tz":-540,"elapsed":2800,"user":{"displayName":"윤세영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08877759417780914923"}},"outputId":"9dedaeb4-4e4d-415e-caf2-ab6d5ec05ffa"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchtext==0.8.0 in /usr/local/lib/python3.7/dist-packages (0.8.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (1.21.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (2.23.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (1.10.0+cu111)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (4.63.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (1.24.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.8.0) (3.10.0.2)\n"]}]},{"cell_type":"code","source":["# 필드 라이브러리를 이용한 데이터셋의 구체적인 전처리 내용 명시\n","from torchtext.data import Field, BucketIterator\n","\n","# 소스 : 독일어\n","SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True)\n","\n","# 목표 : 영어\n","TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0TrYTpVlgZ-2","executionInfo":{"status":"ok","timestamp":1648193251722,"user_tz":-540,"elapsed":323,"user":{"displayName":"윤세영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08877759417780914923"}},"outputId":"65512268-351a-4003-b6fe-4c7a6cb3b231"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"]}]},{"cell_type":"code","source":["# 영어-독어 번역 데이터셋 임포트\n","from torchtext.datasets import Multi30k\n","\n","train_dataset, valid_dataset, test_dataset = Multi30k.splits(exts=('.de', '.en'), fields=(SRC, TRG))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9_EAwA4dboyq","executionInfo":{"status":"ok","timestamp":1648193264588,"user_tz":-540,"elapsed":4691,"user":{"displayName":"윤세영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08877759417780914923"}},"outputId":"76c99bd5-cd0b-4811-a04e-f91b42306da6"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"]}]},{"cell_type":"code","source":["print(f\"학습 데이터셋(training dataset) 크기: {len(train_dataset.examples)}개\")\n","print(f\"평가 데이터셋(validation dataset) 크기: {len(valid_dataset.examples)}개\")\n","print(f\"테스트 데이터셋(testing dataset) 크기: {len(test_dataset.examples)}개\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k0e15wRacHfJ","executionInfo":{"status":"ok","timestamp":1648193268455,"user_tz":-540,"elapsed":403,"user":{"displayName":"윤세영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08877759417780914923"}},"outputId":"71fea452-d988-4813-c26b-567ba7e90a22"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["학습 데이터셋(training dataset) 크기: 29000개\n","평가 데이터셋(validation dataset) 크기: 1014개\n","테스트 데이터셋(testing dataset) 크기: 1000개\n"]}]},{"cell_type":"code","source":["# 학습 데이터 중 하나를 선택해 출력\n","# 훈련 데이터의 src 필드 단어 예시 => 독어 (순서 뒤집기)\n","print(vars(train_dataset.examples[30])['src'])\n","\n","# 훈련 데이터의 trg 필드 단어 예시 => 영어\n","print(vars(train_dataset.examples[30])['trg'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6RPcn6VAcNEl","executionInfo":{"status":"ok","timestamp":1648193271579,"user_tz":-540,"elapsed":7,"user":{"displayName":"윤세영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08877759417780914923"}},"outputId":"e36358b6-cacb-4846-ee2e-5a1855906b35"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["['.', 'steht', 'urinal', 'einem', 'an', 'kaffee', 'tasse', 'einer', 'mit', 'der', ',', 'mann', 'ein']\n","['a', 'man', 'standing', 'at', 'a', 'urinal', 'with', 'a', 'coffee', 'cup', '.']\n"]}]},{"cell_type":"code","source":["# field 객체의 build_vocab 메소드를 이용하여 영어, 독어 단어 사전 생성\n","# 최소 2번 이상 등장한 단어만 선택\n","\n","SRC.build_vocab(train_dataset, min_freq=2)\n","TRG.build_vocab(train_dataset, min_freq=2)\n","\n","print(f'len(SRC): {len(SRC.vocab)}')\n","print(f'len(TRG): {len(TRG.vocab)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jpiuMQlscSgZ","executionInfo":{"status":"ok","timestamp":1648193293307,"user_tz":-540,"elapsed":451,"user":{"displayName":"윤세영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08877759417780914923"}},"outputId":"4ccd9644-3094-473b-dd0a-97be4ef1ac9c"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["<torchtext.vocab.Vocab object at 0x7fd986edad50>\n","len(SRC): 7855\n","len(TRG): 5893\n"]}]},{"cell_type":"code","source":["# 생성된 단어 집합 안에 있는 단어들 확인\n","print(TRG.vocab.stoi['abcabc']) # 없는 단어 0\n","print(TRG.vocab.stoi[TRG.pad_token]) # 패딩(padding) : 1\n","print(TRG.vocab.stoi['<sos>']) # <sos>: 2\n","print(TRG.vocab.stoi['<eos>']) # <eos>: 3\n","print(TRG.vocab.stoi['hello'])\n","print(TRG.vocab.stoi['world'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hMTdzC0GdCTM","executionInfo":{"status":"ok","timestamp":1648193313647,"user_tz":-540,"elapsed":302,"user":{"displayName":"윤세영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08877759417780914923"}},"outputId":"0d60a360-9511-46ac-f84a-8b1e6193cd19"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","1\n","2\n","3\n","4112\n","1752\n"]}]},{"cell_type":"code","source":["'''\n","  - 한 문장에 포함된 단어가 연속적으로 LSTM에 입력되어야 함\n","    1. 하나의 배치에 포함된 문장들이 가지는 단어의 개수가 유사하도록 만들면 좋음\n","    2. 이를 위하여 BucketIterator 사용\n","    3. 배치 크기: 128\n","'''\n","\n","import torch\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","BATCH_SIZE = 128\n","\n","# 일반적인 데이터 로더의 iterator와 유사하게 사용 가능\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","    (train_dataset, valid_dataset, test_dataset),\n","    batch_size=BATCH_SIZE,\n","    device=device\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q7sSeikzdtNJ","executionInfo":{"status":"ok","timestamp":1648193579886,"user_tz":-540,"elapsed":10,"user":{"displayName":"윤세영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08877759417780914923"}},"outputId":"c1392e54-efeb-4918-9fd5-dc0873c1a8f7"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"]}]},{"cell_type":"code","source":["for i, batch in enumerate(train_iterator):\n","  src = batch.src\n","  trg = batch.trg\n","\n","  print(f'first batch size: {src.shape}')\n","\n","  # 현재 배치에 있는 하나의 문장에 포함된 정보 출력\n","  for i in range(src.shape[0]):\n","    print(f'index {i}: {src[i][0].item()}')\n","\n","  # check only first batch\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vVPLHq9af8mW","executionInfo":{"status":"ok","timestamp":1648193740340,"user_tz":-540,"elapsed":421,"user":{"displayName":"윤세영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08877759417780914923"}},"outputId":"8f51e87a-8678-468d-8594-f436fb2b20ec"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["first batch size: torch.Size([26, 128])\n","index 0: 2\n","index 1: 4\n","index 2: 317\n","index 3: 24\n","index 4: 12\n","index 5: 1769\n","index 6: 168\n","index 7: 8\n","index 8: 93\n","index 9: 10\n","index 10: 128\n","index 11: 558\n","index 12: 5\n","index 13: 61\n","index 14: 16\n","index 15: 955\n","index 16: 8\n","index 17: 3\n","index 18: 1\n","index 19: 1\n","index 20: 1\n","index 21: 1\n","index 22: 1\n","index 23: 1\n","index 24: 1\n","index 25: 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"]}]},{"cell_type":"markdown","source":["### 2. 인코더 아키텍처\n","  - 주어진 소스 문장을 context vector로 인코딩\n","  - LSTM은 hidden state와 cell state를 반환\n","  - hyper parameters\n","    - input_dim: 하나의 단어에 대한 원핫 인코딩 차원\n","    - embed_dim: 임베딩 차원\n","    - hidden_dim: 히든 상태 차원\n","    - n_layers: RNN 레이어의 개수\n","    - dropout_ratio: 드롭아웃 비율"],"metadata":{"id":"nyntAQi-hPT8"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","# 인코더 아키텍처 정의\n","class Encoder(nn.Module):\n","  def __init__(self, input_dim, embed_dim, hidden_dim, n_layers, dropout_ratio) -> None:\n","      super().__init__()\n","\n","      # 임베딩은 원-핫 인코딩을 특정 차원의 임베딩으로 매핑하는 레이어\n","      self.embedding = nn.Embedding(input_dim, embed_dim)\n","\n","      # LSTM layer\n","      self.hidden_dim = hidden_dim\n","      self.n_layers = n_layers\n","      self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout_ratio)\n","\n","      # dropout\n","      self.dropout = nn.Dropout(dropout_ratio)\n","\n","  # 인코더는 소스 문장을 입력으로 받아 문맥 벡터를 반환\n","  def forward(self, src):\n","    # src: [단어 개수, 배치 크기]: 각 단어의 인덱스 정보\n","    # embedded: [단어 개수, 배치 크기, 임베딩 차원]\n","    embedded = self.dropout(self.embedding(src))\n","\n","    # outputs: [단어 개수, 배치 크기, 히든 차원]: 현재 단어의 출력 정보\n","    # hidden: [레이어 개수, 배치 크기, 히든 차원]: 현재까지의 모든 단어의 정보\n","    # cell: [레이어 개수, 배치 크기, 히든 차원]: 현재까지의 모든 단어의 정보\n","    outputs, (hidden, cell) = self.rnn(embedded)\n","\n","    # return context vector\n","    return hidden, cell"],"metadata":{"id":"TyTPKIl0g8fh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. 디코더 아키텍처\n","  - 주어진 문맥 벡터를 타겟 문장으로 디코딩\n","  - LSTM은 hidden state와 cell state를 반환\n","  - hyper parameters\n","    - input_dim: 하나의 단어에 대한 원핫 인코딩 차원\n","    - embed_dim: 임베딩(embedding) 차원\n","    - hidden_dim: 히든 상태(hidden state) 차원\n","    - n_layers: RNN 레이어의 개수\n","    - dropout_ratio: 드롭아웃(dropout) 비율"],"metadata":{"id":"BV0gewObnMJB"}},{"cell_type":"code","source":["# 디코더 아키텍처 정의\n","class Decoder(nn.Module):\n","  def __init__(self, output_dim, embed_dim, hidden_dim, n_layers, dropout_ratio) -> None:\n","      super().__init__()\n","\n","      # 임베딩은 원-핫 인코딩 말고 특정 차원의 임베딩으로 매핑하는 레이어\n","      self.embedding = nn.Embedding(output_dim, embed_dim)\n","\n","      # LSTM layer\n","      self.hidden_dim = hidden_dim\n","      self.n_layers = n_layers\n","      self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout_ratio)\n","\n","      # FC layer (인코더와 구조적으로 다른 부분)\n","      self.output_dim = output_dim\n","      self.fc_out = nn.Linear(hidden_dim, output_dim)\n","\n","      # dropout\n","      self.dropout = nn.Dropout(dropout_ratio)\n","\n","  # 디코더는 현재까지 출력된 문장에 대한 정보를 입력으로 받아 타겟 문장을 반환\n","  def forward(self, input, hidden, cell):\n","      # input: [배치 크기]: 단어의 개수는 항상 1개이도록 구현\n","      # hidden: [레이어 개수, 배치 크기, 히든 차원]\n","      # cell = context: [레이어 개수, 배치 크기, 히든 차원]\n","      input = input.unsqueeze(0)\n","      # input: [단어 개수 = 1, 배치 크기]\n","\n","      # embedded: [단어 개수, 배치 크기, 임베딩 차원]\n","      embedded = self.dropout(self.embedding(input))\n","\n","      # output: [단어 개수 = 1, 배치 크기, 히든 차원]: 현재 단어의 출력 정보\n","      # hidden: [레이어 개수, 배치 크기, 히든 차원]: 현재까지의 모든 단어의 정보\n","      # cell: [레이어 개수, 배치 크기, 히든 차원]: 현재까지의 모든 단어의 정보\n","      output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n","\n","      # 단어 개수는 1개이므로 차원 제거\n","      # prediction = [배치 크기, 출력 차원]\n","      prediction = self.fc_out(output.squeeze(0))\n","\n","      # (현재 출력 단어, 현재까지의 모든 단어의 정보, 현재까지의 모든 단어의 정보)\n","      return prediction, hidden, cell"],"metadata":{"id":"Z8Cd84qgnJbs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4. Seq2Seq 아키텍처\n","  - encoder: 주어진 소스 문장을 문맥 벡터로 인코딩\n","  - decoder: 주어진 문맥 벡터를 타겟 문장으로 디코딩\n","  - 디코더는 한 단어씩 넣어서 한 번씩 결과를 구함\n","  - teacher forcing: 디코더의 예측을 다음 입력으로 사용하지 않고, 실제 목표 출력(ground-truth)을 다음 입력으로 사용하는 기법"],"metadata":{"id":"nXBPj0eptKeM"}},{"cell_type":"code","source":["class Seq2Seq(nn.Module):\n","  def __init__(self, encoder, decoder, device) -> None:\n","      super().__init__()\n","\n","      self.encoder = encoder\n","      self.decoder = decoder\n","      self.device = device\n","\n","  # 학습 시, 완전한 형태의 소스 문장, 타겟 문장, teacher_forcing_ration 넣어야 함\n","  def forward(self, src, trg, teacher_forcing_ratio=0.5):\n","    # src: [단어 개수, 배치 크기]\n","    # trg: [단어 개수, 배치 크기]\n","    # 먼저 인코더를 거쳐 문맥 벡터를 추출\n","    hidden, cell = self.encoder(src)\n","\n","    # decoder의 최종 결과를 담을 텐서 객체 생성\n","    trg_len = trg.shape[0] # 단어 개수\n","    batch_size = trg.shape[1] # 배치 크기\n","    trg_vocab_size = self.decoder.output_dim # 출력 차원\n","    outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","\n","    # 첫 번째 입력은 항상 <sos> 토큰\n","    input = trg[0, :]\n","\n","    # 타겟 단어의 개수만큼 반복하여 디코더에 포워딩(forwarding)\n","    for t in range(1, trg_len):\n","      output, hidden, cell = self.decoder(input, hidden, cell)\n","\n","      outputs[t] = output # FC를 거쳐 나온 현재 출력 단어 정보\n","      top1 = output.argmax(1) # 가장 확률이 높은 단어의 인덱스 추출\n","\n","      # teacher_forcing_ratio: 학습 시 실제 목표 출력(ground-truth)을 사용하는 비율\n","      import random\n","      teacher_force = random.random() < teacher_forcing_ratio\n","      input = trg[t] if teacher_force else top1 # 현재의 출력 결과를 다음 입력에 넣기\n","\n","    return outputs"],"metadata":{"id":"IS5UCK28tJDg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5. 학습\n","  - 하이퍼 파라미터 설정 및 모델 초기화"],"metadata":{"id":"W-NUIUaVwyjZ"}},{"cell_type":"code","source":["INPUT_DIM = len(SRC.vocab)\n","OUTPUT_DIM = len(TRG.vocab)\n","ENCODER_EMBED_DIM = 256\n","DECODER_EMBED_DIM = 256\n","HIDDEN_DIM = 512\n","N_LAYERS = 2\n","ENC_DROPOUT_RATIO = 0.5\n","DEC_DROPOUT_RATIO = 0.5"],"metadata":{"id":"QrEnoedNwv7f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 인코더(encoder)와 디코더(decoder) 객체 선언\n","enc = Encoder(INPUT_DIM, ENCODER_EMBED_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT_RATIO)\n","dec = Decoder(OUTPUT_DIM, DECODER_EMBED_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT_RATIO)\n","\n","# Seq2Seq 객체 선언\n","model = Seq2Seq(enc, dec, device).to(device)"],"metadata":{"id":"9-odTj7Ew6bW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from imp import init_builtin\n","# 본 논문의 내용대로 u(-0.08, 0.08)의 값으로 모델 가중치 파라미터 초기화\n","def init_weights(m):\n","  for name, param in m.named_parameters():\n","    nn.init.uniform_(param.data, -0.08, 0.08)\n","\n","model.apply(init_weights)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ufyjOQIGw9he","executionInfo":{"status":"ok","timestamp":1648189801492,"user_tz":-540,"elapsed":294,"user":{"displayName":"윤세영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08877759417780914923"}},"outputId":"53334743-df84-42c4-a264-e6461786116f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (encoder): Encoder(\n","    (embedding): Embedding(7855, 256)\n","    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n","  (decoder): Decoder(\n","    (embedding): Embedding(5893, 256)\n","    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n","    (fc_out): Linear(in_features=512, out_features=5893, bias=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n",")"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["# 학습 및 평가 함수 정의\n","import torch.optim as optim\n","\n","# adam optimizer 학습 최적화\n","optimizer = optim.Adam(model.parameters())\n","\n","# 뒷 부분의 패딩에 대해서는 값 무시\n","TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n","criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"],"metadata":{"id":"FaEhvWI-xa14"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 학습 함수\n","def train(model, iterator, optimizer, criterion, clip):\n","  model.train() # 학습 모드\n","  epoch_loss = 0\n","\n","  # 전체 학습 데이터 확인\n","  for i, batch in enumerate(iterator):\n","    src = batch.src\n","    trg = batch.trg\n","\n","    optimizer.zero_grad()\n","\n","    # output: [출력 단어 개수, 배치 크기, 출력 차원]\n","    output = model(src, trg)\n","    output_dim = output.shape[-1]\n","\n","    # 출력 단어의 인덱스 0은 사용하지 않음\n","    # output = [(출력 단어의 개수 - 1) * batch size, output dim]\n","    output = output[1:].view(-1, output_dim)\n","    # trg = [(타겟 단어의 개수 - 1) * batch size]\n","    trg = trg[1:].view(-1)\n","\n","    # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n","    loss = criterion(output, trg)\n","    loss.backward() # 기울기 계산\n","\n","    # 기울기 clipping 진행\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","\n","    # 파라미터 업데이트\n","    optimizer.step()\n","\n","    # 전체 손실 값 계산\n","    epoch_loss += loss.item()\n","\n","  return epoch_loss / len(iterator)"],"metadata":{"id":"j5_loz0zx05s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 평가 함수\n","def evaluate(model, iterator, criterion):\n","  model.eval() # 평가 모드\n","  epoch_loss = 0\n","\n","  with torch.no_grad():\n","    # 전체 평가 데이터를 확인하며\n","    for i, batch in enumerate(iterator):\n","      src = batch.src\n","      trg = batch.trg\n","\n","      # 평가할 때 teacher forcing은 사용하지 않음\n","      # output: [출력 단어 개수, 배치 크기, 출력 차원]\n","      output = model(src, trg, 0)\n","      output_dim = output.shape[-1]\n","\n","      # 출력 단어의 인덱스 0은 사용하지 않음\n","      # output = [(출력 단어의 개수 - 1) * batch size, output dim]\n","      output = output[1:].view(-1, output_dim)\n","\n","      # trg = [(타겟 단어의 개수 - 1) * batch size]\n","      trg = trg[1:].view(-1)\n","\n","      # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n","      loss = criterion(output, trg)\n","\n","      # 전체 손실 값 계산\n","      epoch_loss += loss.item()\n","\n","  return epoch_loss / len(iterator)"],"metadata":{"id":"oh0GwwQVA2X_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습 및 검증 시행\n","# epoch=20\n","def epoch_time(start_time, end_time):\n","  elapsed_time = end_time - start_time\n","  elapsed_mins = int(elapsed_time / 60)\n","  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","  return elapsed_mins, elapsed_secs"],"metadata":{"id":"qD-uuwLcCRqt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","import math\n","import random\n","\n","N_EPOCHS = 20\n","CLIP = 1\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","  # 시작 시간\n","  start_time = time.time()\n","\n","  train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n","  valid_loss = evaluate(model, valid_iterator, criterion)\n","\n","  # 종료 시간\n","  end_time = time.time()\n","  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","  if valid_loss < best_valid_loss:\n","    best_valid_loss = valid_loss\n","    torch.save(model.state_dict(), 'seq2seq.pt')\n","\n","  print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","  print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n","  print(f'\\tValidation Loss: {valid_loss:.3f} | Validation PPL: {math.exp(valid_loss):.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XiTeRQMADPo9","executionInfo":{"status":"ok","timestamp":1648178889257,"user_tz":-540,"elapsed":540923,"user":{"displayName":"윤세영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08877759417780914923"}},"outputId":"59d346fe-65e2-47e0-b1c4-f483e99036bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Time: 0m 26s\n","\tTrain Loss: 5.065 | Train PPL: 158.352\n","\tValidation Loss: 4.924 | Validation PPL: 137.519\n","Epoch: 02 | Time: 0m 26s\n","\tTrain Loss: 4.508 | Train PPL: 90.767\n","\tValidation Loss: 4.888 | Validation PPL: 132.748\n","Epoch: 03 | Time: 0m 26s\n","\tTrain Loss: 4.170 | Train PPL: 64.738\n","\tValidation Loss: 4.609 | Validation PPL: 100.377\n","Epoch: 04 | Time: 0m 26s\n","\tTrain Loss: 3.961 | Train PPL: 52.491\n","\tValidation Loss: 4.569 | Validation PPL: 96.430\n","Epoch: 05 | Time: 0m 26s\n","\tTrain Loss: 3.784 | Train PPL: 43.998\n","\tValidation Loss: 4.366 | Validation PPL: 78.709\n","Epoch: 06 | Time: 0m 27s\n","\tTrain Loss: 3.639 | Train PPL: 38.036\n","\tValidation Loss: 4.252 | Validation PPL: 70.254\n","Epoch: 07 | Time: 0m 26s\n","\tTrain Loss: 3.497 | Train PPL: 33.023\n","\tValidation Loss: 4.084 | Validation PPL: 59.358\n","Epoch: 08 | Time: 0m 27s\n","\tTrain Loss: 3.346 | Train PPL: 28.399\n","\tValidation Loss: 3.991 | Validation PPL: 54.104\n","Epoch: 09 | Time: 0m 26s\n","\tTrain Loss: 3.216 | Train PPL: 24.926\n","\tValidation Loss: 3.897 | Validation PPL: 49.241\n","Epoch: 10 | Time: 0m 26s\n","\tTrain Loss: 3.095 | Train PPL: 22.077\n","\tValidation Loss: 3.799 | Validation PPL: 44.645\n","Epoch: 11 | Time: 0m 26s\n","\tTrain Loss: 2.977 | Train PPL: 19.632\n","\tValidation Loss: 3.796 | Validation PPL: 44.530\n","Epoch: 12 | Time: 0m 26s\n","\tTrain Loss: 2.886 | Train PPL: 17.928\n","\tValidation Loss: 3.715 | Validation PPL: 41.047\n","Epoch: 13 | Time: 0m 27s\n","\tTrain Loss: 2.790 | Train PPL: 16.276\n","\tValidation Loss: 3.758 | Validation PPL: 42.873\n","Epoch: 14 | Time: 0m 26s\n","\tTrain Loss: 2.674 | Train PPL: 14.494\n","\tValidation Loss: 3.711 | Validation PPL: 40.899\n","Epoch: 15 | Time: 0m 26s\n","\tTrain Loss: 2.626 | Train PPL: 13.813\n","\tValidation Loss: 3.641 | Validation PPL: 38.127\n","Epoch: 16 | Time: 0m 27s\n","\tTrain Loss: 2.519 | Train PPL: 12.412\n","\tValidation Loss: 3.690 | Validation PPL: 40.058\n","Epoch: 17 | Time: 0m 26s\n","\tTrain Loss: 2.452 | Train PPL: 11.608\n","\tValidation Loss: 3.717 | Validation PPL: 41.147\n","Epoch: 18 | Time: 0m 26s\n","\tTrain Loss: 2.397 | Train PPL: 10.995\n","\tValidation Loss: 3.628 | Validation PPL: 37.656\n","Epoch: 19 | Time: 0m 26s\n","\tTrain Loss: 2.315 | Train PPL: 10.129\n","\tValidation Loss: 3.682 | Validation PPL: 39.735\n","Epoch: 20 | Time: 0m 26s\n","\tTrain Loss: 2.267 | Train PPL: 9.655\n","\tValidation Loss: 3.650 | Validation PPL: 38.466\n"]}]},{"cell_type":"code","source":["# 학습된 모델 저장\n","from google.colab import files\n","\n","files.download('seq2seq.pt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"HYJpJYu5HvHW","executionInfo":{"status":"ok","timestamp":1648178961501,"user_tz":-540,"elapsed":291,"user":{"displayName":"윤세영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08877759417780914923"}},"outputId":"1556ec2c-138f-4efa-93df-940a37fc667e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_22e2c1c5-c539-4929-b182-df4a01b30f58\", \"seq2seq.pt\", 55599511)"]},"metadata":{}}]},{"cell_type":"markdown","source":["### 6. 모델 최종 테스트 결과 확인"],"metadata":{"id":"W_5o6__rv9VH"}},{"cell_type":"code","source":["!wget https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/ERgwTMYWR7FMhApROaNvZREBTjEDi00ttSzt8ZNj1PS_5g?download=1 -O seq2seq.pt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_vwBDGQ8v7OJ","executionInfo":{"status":"ok","timestamp":1648189575215,"user_tz":-540,"elapsed":5454,"user":{"displayName":"윤세영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08877759417780914923"}},"outputId":"69b7b5e7-17ac-4554-8950-f773ae7db444"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-03-25 06:26:09--  https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/ERgwTMYWR7FMhApROaNvZREBTjEDi00ttSzt8ZNj1PS_5g?download=1\n","Resolving postechackr-my.sharepoint.com (postechackr-my.sharepoint.com)... 13.107.136.9, 13.107.138.9\n","Connecting to postechackr-my.sharepoint.com (postechackr-my.sharepoint.com)|13.107.136.9|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /personal/dongbinna_postech_ac_kr/Documents/Research/models/seq2seq.pt [following]\n","--2022-03-25 06:26:10--  https://postechackr-my.sharepoint.com/personal/dongbinna_postech_ac_kr/Documents/Research/models/seq2seq.pt\n","Reusing existing connection to postechackr-my.sharepoint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 55600205 (53M) [application/octet-stream]\n","Saving to: ‘seq2seq.pt’\n","\n","seq2seq.pt          100%[===================>]  53.02M  14.5MB/s    in 3.6s    \n","\n","2022-03-25 06:26:14 (14.5 MB/s) - ‘seq2seq.pt’ saved [55600205/55600205]\n","\n"]}]},{"cell_type":"code","source":["import math\n","model.load_state_dict(torch.load('seq2seq.pt'))\n","\n","test_loss = evaluate(model, test_iterator, criterion)\n","\n","print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vo0QeGyqwR_m","executionInfo":{"status":"ok","timestamp":1648189877514,"user_tz":-540,"elapsed":338,"user":{"displayName":"윤세영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08877759417780914923"}},"outputId":"9830715c-d00b-4477-b8b7-99c4eb92f76f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 3.593 | Test PPL: 36.330\n"]}]},{"cell_type":"code","source":["# translation function\n","def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50):\n","  model.eval() # 평가 모드\n","\n","  if isinstance(sentence, str):\n","    nlp = spacy.load('de')\n","    tokens = [token.text.lower() for token in nlp(sentence)]\n","  else:\n","    tokens = [token.lower() for token in sentence]\n","\n","  # 처음에 <sos> 토큰, 마지막에 <eos> 토큰 붙이기\n","  tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n","  print(f'전체 소스 토큰: {tokens}')\n","\n","  src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n","  print(f'소스 문장 인덱스: {src_indexes}')\n","\n","  src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n","\n","  # 인코더에 소스 문장 넣어 문맥 벡터 계산\n","  with torch.no_grad():\n","    hidden, cell = model.encoder(src_tensor)\n","\n","  # 처음에는 <sos> 토큰 하나만 가지고 있도록 함\n","  trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n","\n","  for i in range(max_len):\n","    # 이전에 출력한 단어가 현재 단어로 입력될 수 있도록\n","    trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n","\n","    with torch.no_grad():\n","      output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n","\n","    pred_token = output.argmax(1).item()\n","    trg_indexes.append(pred_token) # 출력 문장에 더하기\n","\n","    # <eos> 만나는 순간 끝\n","    if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n","      break\n","\n","  # 각 출력 단어 인덱스를 실제 단어로 변환\n","  trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n","\n","  # 첫 번째 <sos>는 제외한 출력 문장 반환\n","  return trg_tokens[1:]"],"metadata":{"id":"Uvcv0D6gxbxJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example_idx = 10\n","\n","src = vars(test_dataset.examples[example_idx])['src']\n","trg = vars(test_dataset.examples[example_idx])['trg']\n","\n","print(f'소스 문장: {src}')\n","print(f'타겟 문장: {trg}')\n","print('모델 출력 결과: ', ' '.join(translate_sentence(src, SRC, TRG, model, device)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iOQtuwILzuyN","executionInfo":{"status":"ok","timestamp":1648190608391,"user_tz":-540,"elapsed":372,"user":{"displayName":"윤세영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08877759417780914923"}},"outputId":"5b33101c-96b4-4f3e-d4e5-d512aeddc169"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["소스 문장: ['.', 'freien', 'im', 'tag', 'schönen', 'einen', 'genießen', 'sohn', 'kleiner', 'ihr', 'und', 'mutter', 'eine']\n","타겟 문장: ['a', 'mother', 'and', 'her', 'young', 'song', 'enjoying', 'a', 'beautiful', 'day', 'outside', '.']\n","전체 소스 토큰: ['<sos>', '.', 'freien', 'im', 'tag', 'schönen', 'einen', 'genießen', 'sohn', 'kleiner', 'ihr', 'und', 'mutter', 'eine', '<eos>']\n","소스 문장 인덱스: [2, 4, 88, 20, 200, 780, 19, 565, 624, 70, 134, 10, 364, 8, 3]\n","모델 출력 결과:  a mother and her little boys enjoying a day day day . <eos>\n"]}]},{"cell_type":"code","source":["src = tokenize_de('Guten nacht')\n","\n","print(f'소스 문장: {src}')\n","print('모델 출력 결과: ', ' '.join(translate_sentence(src, SRC, TRG, model, device)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HaSJxxrb0ZWl","executionInfo":{"status":"ok","timestamp":1648190735393,"user_tz":-540,"elapsed":5,"user":{"displayName":"윤세영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08877759417780914923"}},"outputId":"a492056a-7db1-4492-85dd-a8c179963ec8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["소스 문장: ['nacht', 'Guten']\n","전체 소스 토큰: ['<sos>', 'nacht', 'guten', '<eos>']\n","소스 문장 인덱스: [2, 521, 3799, 3]\n","모델 출력 결과:  at night . <eos>\n"]}]}]}